{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 07 Lecture: First Steps with Unstructured Data: PDF Extraction and Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Types__: <BR>\n",
    "* Structured Examples: tables, csv, databases\n",
    "* Unstructured: Images, text files, sound files\n",
    "* Semi-unstructured: json, XML, HTML\n",
    "\n",
    "- This part of the course is concerned with working with unstructured data in the form of text files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Raw Text to NLP-Ready Data\n",
    "*   Within NLP Pre-processing we have 3 core steps:\n",
    "* __Tokenisation__:\n",
    "    - This is the stage in which we break down text into smaller more meaningful portions/units -> which are the tokens\n",
    "    - These smaller sub-units are not sharply defined - so can be words, characters, phrases, sentences, paragraphs etc. -> depends on how we choose to tokenise \n",
    "    - Some methods: word tokenisation -> create a new token every time there is a space in the text, sub-word token -> e.g. breaking dishonesty into dis-honesty -> here we can use methods like Byte-Pair encoding, WordPiece, SentencePiece\n",
    "    - __For our project the pdf-text extraction function does NOT tokenise the text__\n",
    "    - Part of tokenising involves handling special cases and preserving meaning (e.g. dont create seperate tokens in the middle of a word)\n",
    "    - Some Modern NLP Models such as: BERT, GPT, T5 -> use WordPiece, Byte-Pair-Encoding, SentencePiece respectively \n",
    "\n",
    "* __Normalisation__:\n",
    "    - __Requires some domain expertise: e.g. knowing how to convert abbreviations, what's relevant and what is not__\n",
    "    - Normalisation is the prcoss of us standardising words/characters etc. within the tokens to preserve meaning and prevent incorrect differentiation. Ensures all words with the same meaning are treated as the same entity\n",
    "    - Part of this process involves removal of punctuation, standard casing e.g. all lower case. \n",
    "    - Part of normaliation also includes - correcting unicode characters e.g. accented characters, different spellings (colour and color etc.)\n",
    "    - __Lemmatisation__: Can be conceptualised as being part of normalisation -> here this is more robust than stemming - we actually convert words to their dictionary root word  \n",
    "    - __Stemming__: This is the process of removing suffixes from the words -> less precise than lemmatisation. Can result in things like: if we are removing the -es- suffix, then studies becomes studi \n",
    "* __Stop-word removal__:\n",
    "    - This is the removal of 'reundant' non information adding words like fillers. Examples are and, or, with, in, because, if, else, etc. (prepositions, connective ) so non-content words \n",
    "    - This also increases computational efficiency, time of the NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings: From Words to Vectors\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
